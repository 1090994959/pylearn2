Modification in the trunk since the last release
------------------------------------------------
THIS IS A PARITIAL LIST. WE SHOULD CHECK ALL COMMIT SINCE LAST RELEASE AND ADD WHAT IS MISSING.


bugfix:
 * The random number generator in theano/sandbox/rng_mrg.py did not always return the same sequence of number on the cpu and gpu.
    * In that case their was garbadge in the value return, but that garbage looked random. So if you usage did not depend too much
      on the type of random, you can be ok.
 * Memory leak on the gpu when doing x+=y and that x and y are cuda_ndarray.
    * The leak was introduced the 3 December 2010.
    * This was being used when inc_subtensor is called and moved to the GPU(as an GpuIncSubtensor op)
 * In python mode(not the default mode) when input of elemwise operation was an empty ndarray. We where not returning an empty ndarray.
 * Fix some segfault at exit with gpu code.
 * Add a feature to don't have an exception that make theano crash when taking the gradient on DimShuffle in some particular case.
 * Fix compilation crash for gpuElemwise with tensor with very high number of dimensions.
 * Disabled c code generator that make gcc crash on complex type.

optimization:
 * Fuse GpuElemwise more often (in the case where there are so many inputs that fusing them all would bust the 256 bytes limit of parameter to gpu function).
 * Speed up gemv by a work around scipy gemv slowness when the matrix is in C order (the default).
 * remove join of only 1 element
 * fix ticket #596:cpu join of only 1 element that was not moved to the gpu.
 * During optimization consider one more case in get_constant_value
 * new SpecifyShape op that allow to pass more shape info in the graph.

gpu:
 * cuda_shared.value = X now work inplace!
     * cuda_shared_var.set_value(new_ndarray) will overrite the old value inplace in the most common case.
 * allow to create a CudaNdarraySharedVariable from a CudaNdarray.

other:
 * compiledir now include the python version to make it easier for people with many python version
 * tensor.prod now implement the gradient
 * DebugMode now warn if an op declared itself as returning a view of the input but did not do so.
    * This can block other op from being inplace on the same inputs. Could lower the reuse of memory.
 * added theano.tensor.std as a short cut to sqrt(var(input=input, axis=axis)).
 * Sparse.structured_dot now work with both matrice are sparse
 * Sparse type is now supported by the shape op and the ShapeFeature optimizer work correctly with them.
 * new init_gpu_device theano flags.

doc:
 * Documented lib.amdlibm config variable.
 * A new page(was done for 0.3 but error hidded it on the web page) on the memory aliasing contract of Theano.

TODO before new version:
 * shared.value is deprecated, use shared.get_value or shared_set_value!
 * doc in the installation instruction about the new init_gpu_device

Theano 0.3 (2010-11-23)
-----------------------

This is the first major release of Theano since 0.1. Version 0.2 development started internally but it was never advertised as a release.

There have been so many changes since 0.1 that we have lost track of many of them. Below is a *partial* list of changes since 0.1.

 * GPU code using NVIDIA's CUDA framework is now generated for many Ops.
 * Some interface changes since 0.1:
     * A new "shared variable" system to allow reusing memory space between Theano functions.
         * A new memory contract has been formally written for Theano, for people who want to minimize memory copies.
     * The old module system has been deprecated.
     * By default, inputs to a Theano function will not be silently downcasted (e.g. from float64 to float32).
     * An error is now raised when using the result of logical operation on Theano variable in an 'if' (i.e. an implicit call to __nonzeros__).
     * An error is now raised when we receive a non-aligned ndarray as input to a function (this is not supported).
     * An error is raised when the list of dimensions passed to dimshuffle() contains duplicates or is otherwise not sensible.
     * Call NumPy BLAS bindings for gemv operations in addition to the already supported gemm.
     * If gcc is unavailable at import time, Theano now falls back to a Python-based emulation mode after raising a warning.
     * An error is now raised when tensor.grad is called on a non-scalar Theano variable (in the past we would implicitly do a sum on the tensor to make it a scalar).
     * Added support for "erf" and "erfc" functions.
 * The current default value of the parameter axis of theano.{max,min,argmax,argmin,max_and_argmax} is deprecated. We now use the default NumPy behavior of operating on the entire tensor.
 * Theano is now available from PyPI and installable through "easy_install" or "pip".
