
.. currentmodule:: tensor


.. _libdoc_tensor_type:

TensorType
==========

.. class:: TensorType

   .. method:: quux()



.. _libdoc_tensor_creation:

Creation
========

A tensor variable can be created 


Autocasting
-----------

TODO: What does (or compatible) mean?  Talk about casting rules, refer .


.. function:: as_tensor_variable(x, ...)



.. function:: lvector(name=None)

TODO: make a table of all [scalar, vector, matrix, tensor3, tensor4] vs. [b,
w, i, l, f, d, c, z]


Shaping and Shuffling
=====================

.. function:: shape(x)

    :param x:  symbolic Tensor (or compatible)

    Returns the symbolic shape vector of `x`

.. function:: reshape(x)

.. function:: dimshuffle(x)


Reductions 
==========


.. function:: max(x)

    :param x:  symbolic Tensor (or compatible)

    Returns TODO

.. function:: min(x)

    :param x:  symbolic Tensor (or compatible)

    Returns TODO

.. function:: sum(x)

    :param x:  symbolic Tensor (or compatible)

    Returns TODO

Indexing
========

Basic indexing.

Advanced indexing.

.. _libdoc_tensor_elementwise:


Elementwise
===========

Casting
-------

Logic Functions
---------------

Mathematical
------------

.. _libdoc_tensor_broadcastable:

Broadcasting in Theano vs. Numpy
--------------------------------

Broadcasting is a mechanism which allows tensors with
different numbers of dimensions to be added or multiplied
together by (virtually) replicating the smaller tensor along
the dimensions that it is lacking.

In a nutshell, broadcasting is the mechanism by which a scalar
may be added to a matrix, a vector to a matrix or a scalar to
a vector.

.. figure:: bcast.png

Broadcasting a row matrix. T and F respectively stand for
True and False and indicate along which dimensions we allow
broadcasting.

If the second argument were a vector, its shape would be
``(2,)`` and its broadcastable pattern ``(F,)``. They would
be automatically expanded to the **left** to match the
dimensions of the matrix (adding ``1`` to the shape and ``T``
to the pattern), resulting in ``(1, 2)`` and ``(T, F)``.
It would then behave just like the example above.


Unlike numpy which does broadcasting dynamically, Theano needs
to know, for any operation which supports broadcasting, which
dimensions will need to be broadcasted. When applicable, this
information is given in the :ref:`type` of a *Variable*.

See also:

* :ref:`How broadcasting is used in Theano's tensor types <tensortypes>`

* `SciPy documentation about numpy's broadcasting <http://www.scipy.org/EricsBroadcastingDoc>`_

* `OnLamp article about numpy's broadcasting <http://www.onlamp.com/pub/a/python/2000/09/27/numerically.html>`_




Linear Algebra
==============

.. function:: dot(X, Y)

    :param X: left term
    :param Y: right term
    :type X: symbolic matrix or vector
    :type Y: symbolic matrix or vector
    :rtype: symbolic matrix or vector
    :return: the inner product of `X` and `Y`.

.. function:: outer(X, Y)

    :param X: left term
    :param Y: right term
    :type X: symbolic vector
    :type Y: symbolic vector
    :rtype: symbolic matrix 

    :return: vector-vector outer product

.. function:: tensordot(X, Y, axes=2)

    This is a symbolic standing for ``numpy.tensordot``.

    :param X: left term
    :param Y: right term
    :param axes: sum out these axes from X and Y.
    :type X: symbolic tensor
    :type Y: symbolic tensor
    :rtype: symbolic tensor 
    :type axes: see numpy.tensordot

    :return: tensor product



Fourier Transforms
==================

[James has some code for this, but hasn't gotten it into the source tree yet.]

Gradient / Differentiation
==========================

.. function:: grad(cost, wrt, g_cost=None, consider_constant=[], warn_type=False)

    Return symbolic gradients for one or more variables with respect to some
    cost.
    
    :type cost: 0-d tensor variable
    :type wrt: tensor variable or list of tensor variables
    :type g_cost: same as `cost`
    :type consider_constant: list of variables
    :type warn_type: bool

    :param cost: a scalar with respect to which we are differentiating
    :param wrt: term[s] for which we want gradients
    :param g_cost: the gradient on the cost
    :param consider_constant: variables whose gradients will be held at 0.
    :param warn_type: True will trigger warnings via the logging module when
       the gradient on an expression has a different type than the original
       expression

    :rtype: variable or list of variables (matching `wrt`)
    :returns: gradients with respect to cost for each of the `wrt` terms 


